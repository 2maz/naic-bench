pytorch:
  transformerxl_base:
    repo:
      url: https://github.com/2maz/naic-DeepLearningExamples.git
    command: >
      python train.py
    command_distributed: >
      python -m torch.distributed.run --nproc_per_node={{GPU_COUNT}} train.py
    prepare:
      data: transformer.prepare
    metrics:
      throughput:
        pattern: "Training throughput\\s*:\\s*([0-9e\\.+]+)"
    variants:
      fp16:
        base_dir: PyTorch/LanguageModeling/Transformer-XL/pytorch
        batch_size:
          size_1gb:
            default: 1.4
            overrides:
              xpu: 1.2
          multiple_gpu_scaling_factor:
            default: 0.9
            overrides:
              xpu: 0.83
          apply_via: --batch_size
        arguments:
          affinity: disabled
          amp: pytorch
          cuda:
          d_head: 64
          d_inner: 2048
          d_model: 512
          data: "{{DATA_DIR}}/transformer-xl/wikitext-103"
          dataset: wt103
          dropatt: 0.0
          dropout: 0.1
          eta_min: 0.001
          eval_interval: 5000
          eval_tgt_len: 192
          log_interval: 10
          fp16:
          lr: 0.0
          max_step: 400
          mem_len: 192
          n_head: 8
          n_layer: 16
          no_eval:
          optim: jitlamb
          roll:
          tgt_len: 192
          warmup_step: 1000
          work_dir: "{{TMP_DIR}}"
      fp32:
        base_dir: PyTorch/LanguageModeling/Transformer-XL/pytorch
        batch_size:
          # A40
          size_1gb:
            default: 0.95
          multiple_gpu_scaling_factor:
            default: 0.9
            overrides:
              xpu: 0.83
          apply_via: --batch_size
        arguments:
          affinity: disabled
          amp: pytorch
          cuda:
          d_head: 64
          d_inner: 2048
          d_model: 512
          data: "{{DATA_DIR}}/transformer-xl/wikitext-103"
          dataset: wt103
          dropatt: 0.0
          dropout: 0.1
          eta_min: 0.001
          eval_interval: 5000
          eval_tgt_len: 192
          log_interval: 10
          lr: 0.0
          max_step: 400
          mem_len: 192
          n_head: 8
          n_layer: 16
          no_eval:
          optim: jitlamb
          roll:
          tgt_len: 192
          warmup_step: 1000
          work_dir: "{{TMP_DIR}}"

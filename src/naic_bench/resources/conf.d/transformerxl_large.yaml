pytorch:
  transformerxl_large:
    repo:
      url: https://github.com/2maz/naic-DeepLearningExamples.git
    command: >
      python train.py
    command_distributed: >
      python -m torch.distributed.run --nproc_per_node={{GPU_COUNT}} train.py
    prepare:
      data: transformer.prepare
    metrics:
      throughput:
        pattern: "Training throughput\\s*:\\s*([0-9e\\.+]+)"
    variants:
      fp16:
        base_dir: PyTorch/LanguageModeling/Transformer-XL/pytorch
        batch_size:
          size_1gb: 0.25
          multiple_pu_scaling_factor: 1.0
          apply_via: --batch_size
        arguments:
          work_dir: "{{TMP_DIR}}"
          data: "{{DATA_DIR}}/transformer-xl/wikitext-103"
          max_step: 400
          dataset: wt103
          n_layer: 18
          d_model: 1024
          n_head: 16
          d_head: 64
          d_inner: 4096
          dropout: 0.2
          dropatt: 0.2
          optim: adam
          lr: 0.0
          warmup_step: 16000
          tgt_len: 256
          mem_len: 256
          eval_tgt_len: 128
          eval_interval: 5000
          no_eval:
          amp: pytorch
          affinity: disabled
          cuda:
          fp16:
      fp32:
        base_dir: PyTorch/LanguageModeling/Transformer-XL/pytorch
        batch_size:
          size_1gb: 0.125
          multiple_pu_scaling_factor: 1.0
          apply_via: --batch_size
        arguments:
          work_dir: "{{TMP_DIR}}"
          data: "{{DATA_DIR}}/transformer-xl/wikitext-103"
          max_step: 400
          dataset: wt103
          n_layer: 18
          d_model: 1024
          n_head: 16
          d_head: 64
          d_inner: 4096
          dropout: 0.2
          dropatt: 0.2
          optim: adam
          lr: 0.0
          warmup_step: 16000
          tgt_len: 256
          mem_len: 256
          eval_tgt_len: 128
          eval_interval: 5000
          amp: pytorch
          affinity: disabled
          no_eval:
          cuda:
          roll:
